{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a508ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42baae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\",encoding=\"latin-1\")\n",
    "test=pd.read_csv(\"test.csv\",encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba50541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>mi momacita won't let me go to my bf's bball...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>Mom says I have to get a new phone IMMEDIATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>My new car was stolen....by my mother who wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>no hang out with the girls 2day. 2moro, hope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>no movie times for sunday!  Rats will have t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ItemID  Sentiment                                      SentimentText\n",
       "0        1          0                       is so sad for my APL frie...\n",
       "1        2          0                     I missed the New Moon trail...\n",
       "2        3          1                            omg its already 7:30 :O\n",
       "3        4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4        5          0           i think mi bf is cheating on me!!!   ...\n",
       "..     ...        ...                                                ...\n",
       "95      96          0    mi momacita won't let me go to my bf's bball...\n",
       "96      97          0    Mom says I have to get a new phone IMMEDIATE...\n",
       "97      98          0    My new car was stolen....by my mother who wa...\n",
       "98      99          0    no hang out with the girls 2day. 2moro, hope...\n",
       "99     100          0    no movie times for sunday!  Rats will have t...\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0bbf6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train[\"SentimentText\"]\n",
    "Y=train[\"Sentiment\"]\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(X,Y,test_size=.3,stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5bf8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cab7f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dfcbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "porter=PorterStemmer\n",
    "def tokenizer_porter(text):\n",
    "    return[porter.stem(word) for word in text.split()]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324cfd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this twit man is nice :)\n"
     ]
    }
   ],
   "source": [
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "    \"\"\"\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(preprocessor('This!! twit man :) is <b>nice</b>'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02201a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "097d8630",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__preprocessor': [None, preprocessor],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__preprocessor': [None, preprocessor],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8234abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c56dbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "360 fits failed out of a total of 480.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "120 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 390, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 348, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2077, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1330, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1201, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 115, in _analyze\n",
      "    doc = tokenizer(doc)\n",
      "  File \"C:\\Users\\marihan\\AppData\\Local\\Temp\\ipykernel_95608\\241087274.py\", line 5, in tokenizer_porter\n",
      "  File \"C:\\Users\\marihan\\AppData\\Local\\Temp\\ipykernel_95608\\241087274.py\", line 5, in <listcomp>\n",
      "TypeError: stem() missing 1 required positional argument: 'word'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.7432706         nan 0.76051547        nan\n",
      " 0.7512144         nan 0.77183104        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.73801291        nan 0.75797232        nan 0.74278485        nan\n",
      " 0.7658732         nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.72551151        nan\n",
      " 0.74419929        nan 0.72481141        nan 0.74807116        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.74069894        nan 0.75947251        nan\n",
      " 0.74572807        nan 0.765616          nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.72944056        nan 0.74575665        nan 0.73049773        nan\n",
      " 0.75148586        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.71855363        nan\n",
      " 0.73464115        nan 0.71768203        nan 0.73942732        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\marihan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect', TfidfVectorizer()),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(random_state=0))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__C': [1.0, 10.0, 100.0],\n",
       "                          'clf__penalty': ['l1', 'l2'],\n",
       "                          'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__preprocessor': [None,\n",
       "                                                 <function preprocessor at 0x000001FD959AE670>],\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'our...\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x000001FD959AE1F0>,\n",
       "                                              <function tokenizer_porter at 0x000001FD959AE310>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f78c0aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 1.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__preprocessor': <function preprocessor at 0x000001FD959AE670>, 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x000001FD959AE1F0>}\n",
      "Best accuracy: 0.772\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: ' + str(gs_lr_tfidf.best_params_))\n",
    "print('Best accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb549e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test: 0.772\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Accuracy in test: %.3f' % clf.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "260185a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is really bad, I don't like it at all --> 0\n",
      "I love this! --> 1\n",
      ":) --> 1\n",
      "I'm sad... :( --> 0\n"
     ]
    }
   ],
   "source": [
    "twits = [\n",
    "    \"This is really bad, I don't like it at all\",\n",
    "    \"I love this!\",\n",
    "    \":)\",\n",
    "    \"I'm sad... :(\"\n",
    "]\n",
    "\n",
    "preds = clf.predict(twits)\n",
    "\n",
    "for i in range(len(twits)):\n",
    "    print(f'{twits[i]} --> {preds[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45abd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([\"i am sad\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06cb6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import*\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog as fd\n",
    "import cv2\n",
    "from PIL import ImageTk, Image\n",
    "import tkinter\n",
    "\n",
    "\n",
    "# create the root window\n",
    "root = tk.Tk()\n",
    "root[\"bg\"] = \"steelblue\"\n",
    "\n",
    "root.title('Tkinter Open File Dialog')\n",
    "root.resizable(False, False)\n",
    "root.geometry('600x600')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def prediction():\n",
    "    predected_lbl=tk.Label(root,text=\" \",font=('arial bold',20),fg=\"white\",bg=\"steelblue\")\n",
    "    pred = clf.predict([str(TextVar.get())])[0]\n",
    "\n",
    "    lab1=Label(image=photo1,bg=\"steelblue\")\n",
    "    lab2=Label(image=photo2,bg=\"steelblue\")\n",
    "  \n",
    "    if pred==1:\n",
    "        predected_lbl['text']=\"positive\"\n",
    "        predected_lbl['fg']=\"yellow green\"\n",
    "        lab1.pack()\n",
    "        \n",
    "        \n",
    "    if pred==0:\n",
    "        predected_lbl['text']=\"nigative\"  \n",
    "        predected_lbl['fg']=\"red\"\n",
    "        lab2.pack()\n",
    "        \n",
    "    predected_lbl.pack()\n",
    "\n",
    "btn_style=ttk.Style()\n",
    "btn_style.configure('TButton',font=('tahoma',25),foreground = 'steelblue')\n",
    "\n",
    "# open button\n",
    "open_button = ttk.Button(\n",
    "    root,\n",
    "    text='Start',\n",
    "    command=prediction,style=\"TButton\"\n",
    ")\n",
    "TextVar=tkinter.StringVar()\n",
    "Text= tk.Label(root,text='enter feedback',font=('arial bold',30),fg=\"white\",bg=\"steelblue\")\n",
    "Text_entry=ttk.Entry(root,font=('arial',20),width=\"40\",textvariable=str(TextVar))\n",
    "\n",
    "\n",
    "img  = Image.open(\"positive1-removebg-preview.png\")\n",
    "img = img.resize((100,100), Image.ANTIALIAS)\n",
    "photo1=ImageTk.PhotoImage(img)\n",
    "\n",
    "img2  = Image.open(\"negative1-removebg-preview.png\")\n",
    "img2 = img2.resize((100,100), Image.ANTIALIAS)\n",
    "photo2=ImageTk.PhotoImage(img2)\n",
    "\n",
    "'''\n",
    "lab=Label(image=photo).pack()\n",
    "'''\n",
    "#Text.grid(row=0,column=0)\n",
    "Text.pack(pady=15)\n",
    "Text_entry.pack(pady=15)\n",
    "open_button.pack(pady=15)\n",
    "\n",
    "# run the application\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d46fc2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text.pack??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec997c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
